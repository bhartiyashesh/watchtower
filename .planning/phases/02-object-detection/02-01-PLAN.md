---
phase: 02-object-detection
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - object_detector.py
  - config.py
  - requirements.txt
  - .env.example
autonomous: true
requirements: [YOLO-01, YOLO-02, YOLO-03, YOLO-04, YOLO-05]

must_haves:
  truths:
    - "ObjectDetector loads YOLO11n model and exposes an async detect() method"
    - "detect() returns Detection objects with label, confidence, and bounding box for relevant classes only (person, car, cat, dog, package)"
    - "YOLO inference runs in a ThreadPoolExecutor and never blocks the asyncio event loop"
    - "crop_person_bbox() extracts the person bounding box region as JPEG bytes suitable for FaceRecognizer.identify()"
    - "COCO class 28 (suitcase) is mapped to the label 'package' with a documented limitation comment"
    - "YOLO_MODEL_PATH is configurable via environment variable for dev (.pt) vs production (NCNN) switching"
  artifacts:
    - path: "object_detector.py"
      provides: "ObjectDetector class with async detect(), _predict_sync(), crop_person_bbox(), Detection dataclass, RELEVANT_CLASSES mapping"
      min_lines: 100
      exports: ["ObjectDetector", "Detection", "crop_person_bbox", "RELEVANT_CLASSES", "CONFIDENCE_THRESHOLD"]
    - path: "config.py"
      provides: "YOLO_MODEL_PATH configuration entry"
      contains: "YOLO_MODEL_PATH"
    - path: "requirements.txt"
      provides: "ultralytics dependency pinned"
      contains: "ultralytics"
    - path: ".env.example"
      provides: "YOLO_MODEL_PATH example entry"
      contains: "YOLO_MODEL_PATH"
  key_links:
    - from: "object_detector.py"
      to: "asyncio event loop"
      via: "loop.run_in_executor(self._executor, self._predict_sync, frame_bytes)"
      pattern: "run_in_executor.*_executor.*_predict_sync"
    - from: "object_detector.py"
      to: "config.py"
      via: "YOLO_MODEL_PATH used as model_path default or passed by caller"
      pattern: "YOLO_MODEL_PATH"
    - from: "object_detector.py Detection dataclass"
      to: "event_store.py write_event() detections parameter"
      via: "Detection fields match dict keys expected by EventStore.write_event()"
      pattern: "label.*confidence.*bbox"
---

<objective>
Create the ObjectDetector module — a YOLO11n wrapper with async interface via ThreadPoolExecutor.

Purpose: This module is the core new artifact of Phase 2. It wraps YOLO11n's synchronous model.predict() behind an async interface so inference never blocks the event loop. It also provides a crop_person_bbox() function for Phase 3 to route face recognition through YOLO-detected person bounding boxes instead of the full frame.

Output: object_detector.py (new), updated config.py, updated requirements.txt, updated .env.example
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-object-detection/02-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@event_store.py
@face_recognizer.py
@config.py
@requirements.txt
@.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install ultralytics and add YOLO config</name>
  <files>requirements.txt, config.py, .env.example</files>
  <action>
1. Add `ultralytics==8.4.16` to requirements.txt (append after existing entries, maintaining the existing format of package==version or package>=version).

2. Run `pip install ultralytics==8.4.16` to install the package in the active virtualenv.

3. Run `pip check` to verify no dependency conflicts between ultralytics, opencv-python, numpy, and existing packages. Log the output. If conflicts are found, document them but do not change pinned versions — the user will resolve.

4. Add YOLO_MODEL_PATH to config.py Config class:
   ```python
   # YOLO Object Detection
   YOLO_MODEL_PATH: str = os.getenv("YOLO_MODEL_PATH", "yolo11n.pt")
   ```
   Place this after the Storage section, before the validate() method. Do NOT add YOLO_MODEL_PATH to the validate() method — the model file may not exist until first run downloads it.

5. Add YOLO_MODEL_PATH to .env.example with a comment explaining dev vs production values:
   ```
   # YOLO Object Detection
   # Use "yolo11n.pt" for development (PyTorch format)
   # Use "yolo11n_ncnn_model" for Raspberry Pi 4 production (NCNN format)
   YOLO_MODEL_PATH=yolo11n.pt
   ```
  </action>
  <verify>
    <automated>cd /Users/yasheshbharti/Documents/experiments/Ring-Auth/files/smart-lock-system && source venv/bin/activate && python -c "from ultralytics import YOLO; print('ultralytics OK')" && python -c "from config import Config; print(f'YOLO_MODEL_PATH={Config.YOLO_MODEL_PATH}')" && pip check 2>&1 | tail -5</automated>
    <manual>Confirm requirements.txt has ultralytics==8.4.16, config.py has YOLO_MODEL_PATH, .env.example has YOLO_MODEL_PATH entry</manual>
  </verify>
  <done>ultralytics imports successfully, Config.YOLO_MODEL_PATH returns "yolo11n.pt" by default, pip check reports no broken dependencies</done>
</task>

<task type="auto">
  <name>Task 2: Create ObjectDetector module with async detect and person crop</name>
  <files>object_detector.py</files>
  <action>
Create object_detector.py with the following components, following the architecture from 02-RESEARCH.md Pattern 1 and Pattern 2:

1. **Detection dataclass** at module level:
   ```python
   @dataclass
   class Detection:
       label: str
       confidence: float
       bbox_x1: float
       bbox_y1: float
       bbox_x2: float
       bbox_y2: float
   ```

2. **RELEVANT_CLASSES dict** mapping COCO class IDs to project labels:
   ```python
   RELEVANT_CLASSES = {
       0: "person",
       2: "car",
       15: "cat",
       16: "dog",
       28: "package",  # COCO "suitcase" class — closest proxy for delivery packages.
                        # Known limitation: flat envelopes and padded mailers may not be detected.
   }
   ```

3. **CONFIDENCE_THRESHOLD = 0.40** — detections below this confidence are dropped.

4. **ObjectDetector class** with:
   - `__init__(self, model_path: str = "yolo11n.pt")` — loads YOLO model once, creates `ThreadPoolExecutor(max_workers=1)`. Log model loading with logger.info.
   - `async detect(self, frame_bytes: bytes) -> list[Detection]` — gets running loop, dispatches `_predict_sync` via `loop.run_in_executor(self._executor, ...)`. Measures elapsed time with `time.monotonic()`. Logs inference time and detection count. Warns if inference exceeds 300ms (thermal throttling indicator).
   - `_predict_sync(self, frame_bytes: bytes) -> list[Detection]` — decodes frame_bytes via `PIL.Image.open(io.BytesIO(frame_bytes)).convert("RGB")`, converts to numpy array, runs `self.model.predict(img_array, verbose=False)`. Iterates `result.boxes`, filters by CONFIDENCE_THRESHOLD and RELEVANT_CLASSES, builds Detection objects with rounded coordinates. Returns empty list on decode failure (logged via logger.exception).
   - `shutdown(self) -> None` — calls `self._executor.shutdown(wait=True)`.

5. **crop_person_bbox() module-level function**:
   ```python
   def crop_person_bbox(frame_bytes: bytes, detection: Detection) -> bytes | None:
   ```
   Opens frame_bytes as PIL Image, gets image size, clamps bbox coordinates to image bounds, validates box is non-degenerate (width > 0 and height > 0), crops, saves as JPEG bytes (quality=90), returns bytes. Returns None on any error (logged via logger.exception).

6. **detections_to_dicts() module-level function**:
   ```python
   def detections_to_dicts(detections: list[Detection]) -> list[dict]:
   ```
   Converts Detection objects to the dict format that EventStore.write_event() expects: `[{"label": ..., "confidence": ..., "bbox_x1": ..., ...}]`. This is the bridge between Phase 2 output and Phase 1 input — used in Phase 3 pipeline.

Imports required: asyncio, io, logging, time, concurrent.futures.ThreadPoolExecutor, dataclasses.dataclass, numpy, PIL.Image, ultralytics.YOLO.

Do NOT import or reference config.py, event_store.py, or face_recognizer.py — this module is standalone. The caller (Phase 3 pipeline) will pass model_path from config and handle the wiring.
  </action>
  <verify>
    <automated>cd /Users/yasheshbharti/Documents/experiments/Ring-Auth/files/smart-lock-system && source venv/bin/activate && python -c "
from object_detector import ObjectDetector, Detection, crop_person_bbox, detections_to_dicts, RELEVANT_CLASSES, CONFIDENCE_THRESHOLD
print(f'Detection fields: {Detection.__dataclass_fields__.keys()}')
print(f'RELEVANT_CLASSES: {RELEVANT_CLASSES}')
print(f'CONFIDENCE_THRESHOLD: {CONFIDENCE_THRESHOLD}')
print(f'ObjectDetector has detect: {hasattr(ObjectDetector, \"detect\")}')
print(f'ObjectDetector has _predict_sync: {hasattr(ObjectDetector, \"_predict_sync\")}')
print(f'ObjectDetector has shutdown: {hasattr(ObjectDetector, \"shutdown\")}')
print(f'crop_person_bbox callable: {callable(crop_person_bbox)}')
print(f'detections_to_dicts callable: {callable(detections_to_dicts)}')
print('All imports OK')
"</automated>
    <manual>Review object_detector.py for correct RELEVANT_CLASSES mapping, ThreadPoolExecutor(max_workers=1), and run_in_executor pattern</manual>
  </verify>
  <done>object_detector.py exists with ObjectDetector class, Detection dataclass, crop_person_bbox(), detections_to_dicts(), RELEVANT_CLASSES with 5 classes including package mapped from suitcase, CONFIDENCE_THRESHOLD=0.40. All symbols import without error. ThreadPoolExecutor(max_workers=1) used for serialized inference.</done>
</task>

</tasks>

<verification>
1. `python -c "from ultralytics import YOLO; print('OK')"` succeeds
2. `python -c "from config import Config; assert Config.YOLO_MODEL_PATH == 'yolo11n.pt'"` succeeds
3. `python -c "from object_detector import ObjectDetector, Detection, crop_person_bbox, detections_to_dicts, RELEVANT_CLASSES; print('All exports OK')"` succeeds
4. `pip check` reports no broken requirements
5. RELEVANT_CLASSES contains exactly 5 entries: person (0), car (2), cat (15), dog (16), package (28)
6. ObjectDetector.__init__ creates ThreadPoolExecutor(max_workers=1)
7. ObjectDetector.detect is an async method using run_in_executor
8. crop_person_bbox returns bytes|None
</verification>

<success_criteria>
- object_detector.py exists with all required classes and functions
- ultralytics installed and importable
- Config.YOLO_MODEL_PATH defaults to "yolo11n.pt"
- No dependency conflicts reported by pip check
- Module is standalone — no imports from config, event_store, or face_recognizer
</success_criteria>

<output>
After completion, create `.planning/phases/02-object-detection/02-01-SUMMARY.md`
</output>
